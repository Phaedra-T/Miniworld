import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# ===================================
# PPO Neural Network Architecture
# ===================================
class MLP_PPO_LSTM(nn.Module):
    def __init__(self, input_dim, num_actions, hidden_size=128):
        super(MLP_PPO_LSTM, self).__init__()

        # Shared MLP encoder
        self.shared = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Linear(256, hidden_size),
            nn.ReLU()
        )

        # Add LSTM layer
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)

        # Actor head (policy)
        self.actor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_actions)
        )

        # Critic head (value function)
        self.critic = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, x, hidden=None):
        # x shape: (batch, seq_len, input_dim) for LSTM
        batch_size, seq_len, _ = x.size()
        x_flat = x.view(batch_size * seq_len, -1)
        shared_features = self.shared(x_flat)
        shared_features = shared_features.view(batch_size, seq_len, -1)
        
        lstm_out, hidden = self.lstm(shared_features, hidden)
        hidden = (hidden[0].detach(), hidden[1].detach())
        
        logits = self.actor(lstm_out)
        value = self.critic(lstm_out)
        return logits, value, hidden

    def init_hidden(self, batch_size=1, device="mps"):
        h = torch.zeros(1, batch_size, 128, device=device)
        c = torch.zeros(1, batch_size, 128, device=device)
        return (h, c)


# ===================================
# PPO Agent Class
# ===================================
class PPOAgent:
    def __init__(self, input_dim, num_actions, device, lr=3e-4, gamma=0.99, clip_eps=0.2, epochs=4):
        self.device = device
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.epochs = epochs

        self.model = MLP_PPO_LSTM(input_dim, num_actions).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)

        # PPO buffers
        self.obs_buf, self.action_buf, self.logprob_buf = [], [], []
        self.reward_buf, self.done_buf, self.value_buf = [], [], []

    # ----------------------------
    # Action selection
    # ----------------------------
# --- Paste into ppo_agent.py replacing the corresponding methods in PPOAgent ---

    def act(self, state, hidden):
        # build input tensor as (batch=1, seq=1, input_dim)
        state_t = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0).unsqueeze(1)

        # forward without building gradient graph
        with torch.no_grad():
            logits, value, new_hidden = self.model(state_t, hidden)

            # logits: (1, 1, num_actions) -> squeeze seq dim
            logits = logits.squeeze(1)   # (1, num_actions)
            value = value.squeeze(1)     # (1,) or (1,1) depending on forward

            probs = torch.distributions.Categorical(logits=logits)
            action = probs.sample()
            log_prob = probs.log_prob(action)

        return int(action.item()), float(log_prob.item()), float(value.item()), new_hidden


    def remember(self, obs, action, logprob, reward, done, value):
        # store numpy (detached) to avoid holding tensors/graphs
        # ensure obs is a numpy array
        if isinstance(obs, np.ndarray):
            self.obs_buf.append(obs.copy())
        else:
            self.obs_buf.append(np.array(obs, dtype=np.float32))
        self.action_buf.append(int(action))
        self.logprob_buf.append(float(logprob))
        self.reward_buf.append(float(reward))
        self.done_buf.append(bool(done))
        self.value_buf.append(float(value))


    def clear_memory(self):
        self.obs_buf = []
        self.action_buf = []
        self.logprob_buf = []
        self.reward_buf = []
        self.done_buf = []
        self.value_buf = []


    def update(self):
        # don't update if not enough rollout
        if len(self.obs_buf) < 256:
            return

        # compute returns & advantages (same as before)
        rewards = np.array(self.reward_buf, dtype=np.float32)
        dones = np.array(self.done_buf, dtype=np.float32)
        values = np.array(self.value_buf, dtype=np.float32)

        advantages, returns = [], []
        gae = 0.0
        next_value = 0.0
        for i in reversed(range(len(rewards))):
            delta = rewards[i] + self.gamma * next_value * (1.0 - dones[i]) - values[i]
            gae = delta + self.gamma * 0.99 * (1.0 - dones[i]) * gae
            advantages.insert(0, gae)
            returns.insert(0, gae + values[i])
            next_value = values[i]

        advantages = np.array(advantages, dtype=np.float32)
        returns = np.array(returns, dtype=np.float32)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # convert obs list -> stacked numpy -> tensor once (fast)
        obs_np = np.stack(self.obs_buf, axis=0)  # shape (T, input_dim)
        obs_t = torch.tensor(obs_np, dtype=torch.float32, device=self.device).unsqueeze(0)  # (1, T, input_dim)
        hidden = self.model.init_hidden(batch_size=1, device=self.device)
        actions_t = torch.tensor(self.action_buf, dtype=torch.long, device=self.device)      # (T,)
        old_logprobs_t = torch.tensor(self.logprob_buf, dtype=torch.float32, device=self.device)  # (T,)
        returns_t = torch.tensor(returns, dtype=torch.float32, device=self.device)
        advs_t = torch.tensor(advantages, dtype=torch.float32, device=self.device)

        for _ in range(self.epochs):
            # forward through model: returns logits (T,1,num_actions), values (T,1,1)
            logits, values, _ = self.model(obs_t, hidden)

            # squeeze sequence dim so shapes align
            logits = logits.squeeze(0)  # (T, num_actions)
            values = values.squeeze(0).squeeze(-1)  # (T,)

            probs = torch.distributions.Categorical(logits=logits)
            new_logprobs = probs.log_prob(actions_t)
            entropy = probs.entropy().mean()

            ratio = torch.exp(new_logprobs - old_logprobs_t)
            surr1 = ratio * advs_t
            surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advs_t
            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = nn.MSELoss()(values, returns_t)
            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy

            self.optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
            self.optimizer.step()

        # housekeeping
        self.clear_memory()

        # free MPS cache (safe)
        try:
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
        except Exception:
            pass


    # ----------------------------
    # Layer freezing
    # ----------------------------
    def freeze_shared(self, freeze=True, partial=False):
        layers = list(self.model.shared.children())
        if partial:
            to_freeze = layers[:2]  # freeze first two layers only
        else:
            to_freeze = layers
        for layer in to_freeze:
            for param in layer.parameters():
                param.requires_grad = not freeze

    # ----------------------------
    # Model saving/loading
    # ----------------------------
    def save(self, path):
        torch.save(self.model.state_dict(), path)

    def load(self, path):
        self.model.load_state_dict(torch.load(path, map_location=self.device))

